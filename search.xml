<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫笔记（二）_urllib+selenium利用cookie]]></title>
    <url>%2F2018%2F10%2F26%2Fpython%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-urllib-selenium%E5%88%A9%E7%94%A8cookie%2F</url>
    <content type="text"><![CDATA[Python爬虫笔记（二）_urllib+selenium利用cookie 备注：主要内容来自互联网，经过修改与整理使得更加符合LCL的阅读习惯。Lcl的习惯：章节号##，副标题###，其他事项#### 本教程可能分为以下几个部分：第一部分：使用urllib发送和处理简单请求第二部分：urllib+selenium利用cookie第三部分：beautifulSoup与正则表达式第四部分：高级爬虫（分布式与框架初探) 第四章使用headers与proxy代理核心代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import urllib.requestif __name__ == "__main__": #访问网址 urls = ['http://httpbin.org/get','https://httpbin.org/get'] #这是代理IP，http与https有各自的ip proxy = &#123; 'http':'109.207.59.70:30668', 'https':'107.150.122.73:3128' &#125; #创建ProxyHandler proxy_handler = urllib.request.ProxyHandler(proxy) #创建Opener opener = urllib.request.build_opener(proxy_handler) #定义User Agent header = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36' &#125; for url in urls: #在请求头中加入user-agent req = urllib.request.Request(url,headers=header) #使用自定义的Opener打开请求 response = opener.open(req,timeout=10) #读取相应信息并解码 html = response.read().decode("utf-8") #打印信息 print(html)'''输出&#123; "args": &#123;&#125;, "headers": &#123; "Accept-Encoding": "identity", "Connection": "close", "Host": "httpbin.org", "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36" &#125;, "origin": "109.207.59.70", "url": "http://httpbin.org/get"&#125;&#123; "args": &#123;&#125;, "headers": &#123; "Accept-Encoding": "identity", "Connection": "close", "Host": "httpbin.org", "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36" &#125;, "origin": "107.150.122.73", "url": "https://httpbin.org/get"&#125;''' 加入header的时候，建议使用urllib.request.Request()对象 加入headers并使用代理建议使用这个网站获取浏览器请求和响应头的关键信息（http://httpbin.org/get 注：这个网站也有https版本，将http改成https即可，访问此网站主页，可以获得更多测试工具，如IP等 ） 新内容解释：urllib.request.Request()构造一个完整的请求，加入header的时候，建议传入headers={}字典。而不是创建后再req.add_header class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)1234data = urllib.parse.urlencode(formData(字典对象)).encode('utf-8')headers = &#123;&#125; #字典对象req = urllib.request.Request(url,data,headers）req.add_header('key','value') 测试时使用的网站 http://httpbin.org/get建议使用这个网站获取浏览器请求和响应头的关键信息注：这个网站也有https版本，将http改成https即可，访问此网站主页，可以获得很多测试工具。 在远程或本地搭建DWVA 使用代理访问并设置header的步骤总结如下1234567#构造requestreq = urllib.request.Request(url,data,headers=&#123;&#125;)#→opener = urllib.request.build_opener(urllib.request.ProxyHandler(为http与https建立的代理dict))#→reasponse = opener.open(req)html = response.read().decode("相应的编码") 附录：常见的user-agent AndroidMozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 FirefoxMozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 Google ChromeMozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 iOSMozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 第五章利用cookie现有的网站登陆普遍需要cookie。而现有的网站验证机制比较复杂，不妨在云服务器上搭建dwva环境供自己测试。 核心代码：12345678910111213141516171819202122232425262728import urllib.requestimport urllib.parsefrom http import cookiejarimport reif __name__ == '__main__': #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() #利用urllib.request库的HTTPCookieProcessor对象来创建cookie处理器,也就是CookieHandler handler= urllib.request.HTTPCookieProcessor(cookie) #通过CookieHandler创建opener opener = urllib.request.build_opener(handler) #此处的open方法打开网页 url = 'http://118.25.194.91/dvwa-master/login.php' #尝试获取csrf-token response = opener.open(url).read().decode('utf-8') str = re.findall(r"name='user_token' value='(.*?)'",response) data = &#123; 'username':'admin', 'password':'password', 'Login':'Login', 'user_token':str[0], &#125; data = urllib.parse.urlencode(data).encode('utf-8') req = urllib.request.Request(url,data=data) response = opener.open(req).read().decode('utf-8') print(response) 新内容解释：opener与cookie的关系获取的cookie默认与 打开此网页的opener关联，所以需要使用获取cookie的opener打开需要登录的网页 第六章同时使用proxy与cookie（多个handler）1234567891011121314151617181920212223242526272829303132333435363738394041424344import urllib.requestimport urllib.parsefrom http import cookiejarimport reif __name__ == '__main__': #创建ProxyHandler proxy = &#123; 'http':'195.78.101.185:48942', &#125; proxyHandler = urllib.request.ProxyHandler(proxy) #创建CookieHandler #声明一个CookieJar对象实例来保存cookie cookie = cookiejar.CookieJar() cookieHandler= urllib.request.HTTPCookieProcessor(cookie) #创建同时包括ProxyHandler与CookieHandler的opener opener = urllib.request.build_opener(cookieHandler,proxyHandler) # 获取csrf-token进行模拟登陆 url = 'http://118.25.194.91/dvwa-master/login.php' response = opener.open(url,timeout=10).read().decode('utf-8') str = re.findall(r"name='user_token' value='(.*?)'",response) # 进行模拟登陆，登陆后cookie会通过cookieHandler传回去 data = &#123; 'username':'admin', 'password':'password', 'Login':'Login', 'user_token':str[0], &#125; data = urllib.parse.urlencode(data).encode('utf-8') req = urllib.request.Request(url,data=data) response = opener.open(req,timeout=10).read().decode('utf-8') # 访问一个可以显示当前IP的页面 url = 'http://118.25.194.91/dvwa-master/vulnerabilities/fi/?page=file3.php' response = opener.open(url,timeout=10).read().decode('utf-8') str = re.findall(r"Your IP address is:(.*?)&lt;h2&gt;More info&lt;/h2&gt;",response,re.DOTALL) req = urllib.request.Request(url) response = opener.open(req,timeout=10).read().decode('utf-8') print(str) 新内容解释：可以使用多个handler创建一个openeropener = urllib.request.build_opener(cookieHandler,proxyHandler) 第七章使用selenium获取cookie并继续工作1、仅使用selenium使用selenium，使用chrome的headless模式静默模拟登陆获取cookie，关闭后重新使用selenium直接获取cookie进行模拟登陆 123456789101112131415161718192021222324252627282930313233343536373839from selenium import webdriverimport time#使用headless模式打开浏览器from selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument('--headless')#声明浏览器使用的驱动，并打开一个空白的浏览器driver = webdriver.Chrome("chromedriver.exe",options=chrome_options)#使用打开的chrome打开地址loginurl = 'http://118.25.194.91/dvwa-master/index.php'driver.get(loginurl)time.sleep(1)#往表单中填充信息并点击登录username = driver.find_element_by_name("username")username.send_keys('admin')password = driver.find_element_by_name("password")password.send_keys('password')login = driver.find_element_by_name("Login")login.click()time.sleep(1)print(driver.current_url)#获得登录后的cookielist_cookies = driver.get_cookies()cookies = []driver.quit()time.sleep(1)#在上文关闭浏览器后，重新打开浏览器，不使用headless模式driver2 = webdriver.Chrome("chromedriver.exe")#必须要先打开一次等待登录的界面，（此时当然是未登录的）才可以完成后续的add_cookie，不然会报错driver2.get(loginurl)for cookie in list_cookies: driver2.add_cookie(cookie)#重新打开需要登录的界面，发现已经登录了driver2.get(loginurl) 2 使用selenium+手工，完成百度的模拟登陆研究的问题：使用selenium打开浏览器后，python端暂停，人工完成登陆，selenium是否可以获得cookie并用于下次的模拟登陆？经验证，可以，验证方式如下↓ 123456789101112131415161718192021222324252627282930from selenium import webdriverimport timeimport json#使用headless模式打开浏览器from selenium.webdriver.chrome.options import Optionschrome_options = Options()#使用隐身模式，隔绝外部影响chrome_options.add_argument('--incognito')#不加载图片#prefs = &#123;"profile.managed_default_content_settings.images": 2&#125;#chrome_options.add_experimental_option("prefs", prefs)#声明浏览器使用的驱动，并打开一个空白的浏览器driver = webdriver.Chrome("chromedriver.exe",options=chrome_options)#使用打开的chrome打开地址loginurl = 'https://www.baidu.com/'driver.get(loginurl)cookie_before = driver.get_cookies()if input("自己完成模拟登陆后，请输入小写的ok：") == "ok": print("已经保存登陆后的cookie，即将关闭并重新打开浏览器并载入cookie") cookie_after = driver.get_cookies() driver.quit() time.sleep(1) driver = webdriver.Chrome("chromedriver.exe",options=chrome_options) driver.get(loginurl) for cookie in cookie_after: driver.add_cookie(cookie) cookie_after_addin = driver.get_cookies() driver.get(loginurl) 新内容解释：json与str的转换list或者dict→→json.dumps(list)→str→→json.loads(str)→→list或者dict助记：json.loads，载入json状的字符串str供计算机使用，比如str→dict而json.dumps 转储，将计算机使用的格式转储为json状字符串str，比如json→dict selenium常用配置（使用隐身模式并不加载图片）from selenium.webdriver.chrome.options import Optionschrome_options = Options() #使用隐身模式，隔绝外部影响chrome_options.add_argument(‘–incognito’) #不加载图片prefs = {“profile.managed_default_content_settings.images”: 2}chrome_options.add_experimental_option(“prefs”, prefs) driver.add_cookie的原理以百度cookie的修改方式举例新增原来不存在的字段，若新增name-value，将 自动补全其他字段，如expiry等。12driver.add_cookie(&#123;'name':"test","value":"value-test"&#125;)&#123;'domain': 'www.baidu.com', 'expiry': 2170944786, 'httpOnly': False, 'name': 'test', 'path': '/', 'secure': True, 'value': 'value-test'&#125; 若修改原来已经存在的字段，也会增添新字段。除name-value字段外，其他将被自动补全。若再次新增同name字段，将仅修改后添加的而不新增。123&#123;'domain': '.www.baidu.com', 'expiry': 2486310942, 'httpOnly': False, 'name': 'ORIGIN', 'path': '/', 'secure': True, 'value': '2'&#125;#↓driver.add_cookie(&#123;'name':"ORIGIN","value":"value-test"&#125;) 第八章selenium进行复杂登录并配合urllib工作了解LWPCookie的标准格式以LWPCookie举例，首先使用标准库生成一个标准的LWP-COOKIE格式123456789101112131415161718192021import urllib.requestimport urllib.parsefrom http import cookiejarimport reif __name__ == '__main__': cookie = cookiejar.LWPCookieJar() handler= urllib.request.HTTPCookieProcessor(cookie) opener = urllib.request.build_opener(handler) url = 'http://118.25.194.91/dvwa-master/login.php' response = opener.open(url).read().decode('utf-8') str = re.findall(r"name='user_token' value='(.*?)'",response) data = &#123; 'username':'admin', 'password':'password', 'Login':'Login', 'user_token':str[0], &#125; data = urllib.parse.urlencode(data).encode('utf-8') req = urllib.request.Request(url,data=data) response = opener.open(req).read().decode('utf-8') cookie.save('realLWP.txt',True,True) realLWP.txt123#LWP-Cookies-2.0Set-Cookie3: PHPSESSID=oetigbp2v5esgdped1vved2ho1; path="/"; domain="118.25.194.91"; path_spec; discard; httponly=None; version=0Set-Cookie3: security=impossible; path="/dvwa-master"; domain="118.25.194.91"; discard; httponly=None; version=0 经测试，部分字段可以删除（如path_spec;等），调整为最简格式如下，格式说明：第一行必须为#LWP-Cookies-2.0第二行开始，开头必须为name=value，且不加引号;domain与path顺序可以替换，最后必须为version=0，最后是否加分号;都不会报错。123#LWP-Cookies-2.0Set-Cookie3:PHPSESSID=oetigbp2v5esgdped1vved2ho1;domain="118.25.194.91";path="/";version=0Set-Cookie3:security=impossible;path="/dvwa-master";domain="118.25.194.91";version=0 将selenium获得的cookie与LWPCookie的标准格式进行转换,并成功模拟淘宝网登录driver.get_cookies()seleium-cookie（已经从list转为str）12[&#123;"domain": "118.25.194.91", "httpOnly": true, "name": "security", "path": "/dvwa-master", "secure": false, "value": "impossible"&#125;, &#123;"domain": "118.25.194.91", "httpOnly": true, "name": "PHPSESSID", "path": "/", "secure": false, "value": "nuoncm7tu449fa66t766282am2"&#125;] #LWPCookie最简123#LWP-Cookies-2.0Set-Cookie3:security=impossible;domain="118.25.194.91";path="/dvwa-master";version=0Set-Cookie3:PHPSESSID=nuoncm7tu449fa66t766282am2;domain="118.25.194.91";path="/";version=0 附上代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import urllib.requestimport urllib.parseimport urllib.errorimport http.cookiejarfrom selenium import webdriverimport json#使用headless模式打开浏览器from selenium.webdriver.chrome.options import Optionschrome_options = Options()#使用隐身模式，隔绝外部影响chrome_options.add_argument('--incognito')#声明浏览器使用的驱动，并打开一个空白的浏览器driver = webdriver.Chrome("chromedriver.exe",options=chrome_options)#使用打开的chrome打开地址loginurl = 'https://member1.taobao.com/member/fresh/account_security.htm'driver.get(loginurl)if input("自行完成登陆后，请输入小写的ok：") == "ok": print("已经保存登陆后的cookie，即将关闭浏览器并使用urllib完成后续工作") cookie_after = driver.get_cookies() driver.quit() with open("selCookie.txt",mode='w') as f: f.write(json.dumps(cookie_after))## 使用urllib继续后续工作 # 定义一个转换cookie格式的函数def selCookie2LWP(selCookie,LWPCookie): import json with open(selCookie) as f: selCookieStr = f.read() selCookieList = json.loads(selCookieStr) cookieList = ['Set-Cookie3:&#123;&#125;=&#123;&#125;;domain="&#123;&#125;";path="&#123;&#125;";version=0\r\n'.format(item["name"],item["value"],item["domain"],item["path"]) for item in selCookieList] LWPCookieStr = '#LWP-Cookies-2.0\r\n' for cookie in cookieList: LWPCookieStr+=cookie with open(LWPCookie,mode='w') as f: f.write(LWPCookieStr)selCookie = "selCookie.txt"LWPCookie = "LWPCookie.txt"# 调用函数完成cookie的格式转换selCookie2LWP(selCookie,LWPCookie)cookie = http.cookiejar.LWPCookieJar(LWPCookie)cookie.load(LWPCookie, ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)get_url = loginurl # 利用cookie请求访问另一个网址get_request = urllib.request.Request(get_url)get_response = opener.open(get_request)html = get_response.read().decode('gb2312',errors='ignore')print(html)# 既然已经能灵活运用cookie了，就可以为所欲为了。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫笔记（一）_使用urllib发送和处理简单请求]]></title>
    <url>%2F2018%2F10%2F25%2Fpython%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-%E4%BD%BF%E7%94%A8urllib%E5%8F%91%E9%80%81%E5%92%8C%E5%A4%84%E7%90%86%E7%AE%80%E5%8D%95%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[备注：主要内容来自互联网，经过修改与整理使得更加符合LCL的阅读习惯。Lcl的习惯：章节号##，副标题###，其他事项#### 本教程可能分为以下几个部分：第一部分：使用urllib发送和处理简单请求第二部分：urllib+selenium利用cookie第三部分：beautifulSoup与正则表达式第四部分：高级爬虫（分布式与框架初探) 第一章利用urllib进行简单的网页抓取核心代码123456from urllib import requestimport chardetresponse = request.urlopen("https://www.163.com/")html = response.read()#读取出来的是bytescharset = chardet.detect(html)html = html.decode(charset['encoding'],errors='ignore') 新内容解释1、chardet：为第三方模块，可以检测byte的编码，若使用Anaconda，则已经自带，该模块可以用来自动判断网页编码的类型。该模块在实际使用时偶尔会碰到问题，即无法检测部分网页的编码，如腾讯主页 2、什么时候使用encode，什么时候使用decode?str—&gt;(encode，编码)—&gt;bytes（二进制）bytes（二进制）—&gt;(decode，解码)—&gt;str助记：将字符编码为计算机理解的二进制语言。举例：str(小明abc）—&gt;str.encode(‘utf-8’,errors=’ignore’)—&gt;bytes（b’\xe5\xb0\x8f\xe6\x98\x8eabc’ 小明abc）bytes（b’\xe5\xb0\x8f\xe6\x98\x8eabc’ 小明abc）—&gt;str.decode(‘utf-8’,errors=’ignore’)—&gt;str(小明abc） 练习：文件编码转换写法1：使用python的with open语句完成文件编码转换，需要注意的是，如果f.read()运行完后指针会到最后，如果需要重复读取可以将其赋值为中间变量。123456from urllib import requestimport chardetwith open('cookie.txt',encoding='gb2312',errors='ignore',newline='\r\n') as f: a = f.read() with open('utf-8.txt',mode='wb') as g: g.write(a.encode('utf-8')) 写法2：（读取写入均为字节，在中间转码。推荐使用这种方式，因为这种方式不需要处理换行问题。）：123456from urllib import requestimport chardetwith open('cookie.txt',mode = 'rb') as f: a = f.read() with open('utf-8.txt',mode='wb') as g: g.write(a.decode(chardet.detect(a)['encoding']).encode('utf-8')) 第二章使用post发送数据核心代码：12345678910111213141516171819202122232425import urllibimport jsonrequestURL = 'http://fanyi.youdao.com/translate'formData = &#123; 'i':'我爱你', 'from':'AUTO', 'to':'AUTO', 'smartresult':'dict', 'client':'fanyideskweb', 'salt':1539952880862, 'sign':'3e6b698c9b9863f602a0eaab4cbdb567', 'doctype':'json', 'version':2.1, 'keyfrom':'fanyi.web', 'action':'FY_BY_CLICKBUTTION', 'typoResult':'false'&#125;#urlencode方法将请求的data转换成URL编码的String转换标准格式，encode将string转换为bytesdata = urllib.parse.urlencode(formData).encode('utf-8')response = urllib.request.urlopen(requestURL,data)#传递完格式的数据html = response.read().decode('utf-8')#读取信息并解码translate_results = json.loads(html)#str→dictprint("翻译的结果是：&#123;&#125;".format(translate_results)) 新内容解释：urllib.request.urlopen( )主要作用是使用默认的opener打开网页，建议设置timeout限制其最大读取时间urllib.request.urlopen(url, data=None[, timeout ], cafile=None, capath=None, cadefault=False,context=None)Open the URL url, which can be either a string or a Request object.data must be an object specifying additional data to be sent to the server, or None if no such data isneeded. urllib.parse.urlencode(可传入dict).encode(‘utf-8’)主要作用是将dict转换为url编码后的string，再将string转换为bytes供下一步调用，例如12345678formData = &#123; 'i':'我爱你', 'from': 'AUTO',&#125;data = urllib.parse.urlencode(formData).encode('utf-8')b'i=%E6%88%91%E7%88%B1%E4%BD%A0&amp;from=AUTO'#如果不加 str.encode('utf-8')，则返回'i=%E6%88%91%E7%88%B1%E4%BD%A0&amp;from=AUTO' Convert a mapping object or a sequence of two-element tuples, which may contain str or bytesobjects, to a percent-encoded ASCII text string. If the resultant string is to be used as a data forPOST operation with the urlopen() function, then it should be encoded to bytes, otherwise it wouldresult in a TypeError. 第三章error异常处理核心代码：123456789101112131415import urllibif __name__ == "__main__": url = "http://www.baidu.com/lcl.html" req = urllib.request.Request(url) try: responese = urllib.request.urlopen(req) except urllib.error.HTTPError as e: print(e.code) except urllib.error.URLError as e: print(e) else: print('如果try中的部分成功了，运行这个') finally: print('无论是否成功，均运行这里') 新内容解释：try…except…except…else…finally12345678910try: print('先尝试运行这里')except urllib.error.HTTPError as e: print('再尝试运行这里')except urllib.error.URLError as e: print('然后尝试运行这里')else: print('如果try中的部分成功了，运行这个')finally: print('无论是否成功，均运行这里') URLError与HTTPError的关系urllib.error.URLError（处理此模块的所有异常）→衍生到子类→urllib.error.HTTPError（仅处理存在错误代码的HTTP出错信息，例如404(找不到页面)）。如果想用HTTPError和URLError一起捕获异常，那么需要将HTTPError放在URLError的前面，因为HTTPError是URLError的一个子类。 本部分主要参考资料： Jack_Gui的CSDN博客 Python library reference_3.6.4]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
</search>
